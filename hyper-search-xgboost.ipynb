{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T19:59:33.307026Z","iopub.execute_input":"2022-05-17T19:59:33.307516Z","iopub.status.idle":"2022-05-17T19:59:33.342016Z","shell.execute_reply.started":"2022-05-17T19:59:33.307386Z","shell.execute_reply":"2022-05-17T19:59:33.340937Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/store-sales-time-series-forecasting/oil.csv\n/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n/kaggle/input/store-sales-time-series-forecasting/stores.csv\n/kaggle/input/store-sales-time-series-forecasting/train.csv\n/kaggle/input/store-sales-time-series-forecasting/test.csv\n/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import IPython\nimport IPython.display\nfrom pathlib import Path\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\ncomp_dir = Path('../input/store-sales-time-series-forecasting')","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:15.751846Z","iopub.execute_input":"2022-05-17T20:03:15.752161Z","iopub.status.idle":"2022-05-17T20:03:16.081183Z","shell.execute_reply.started":"2022-05-17T20:03:15.752128Z","shell.execute_reply":"2022-05-17T20:03:16.080185Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"store_sales = pd.read_csv(\n    comp_dir / 'train.csv',\n    usecols=['store_nbr', 'family', 'date', 'sales'], #, 'onpromotion'],\n    dtype={\n        'store_nbr': 'category',\n        'family': 'category',\n        'sales': 'float32',\n       # 'onpromotion': 'uint32',\n    },\n    parse_dates=['date'],\n    infer_datetime_format=True,\n)\nstore_sales['date'] = store_sales.date.dt.to_period('D')\nstore_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:16.366353Z","iopub.execute_input":"2022-05-17T20:03:16.366682Z","iopub.status.idle":"2022-05-17T20:03:21.614662Z","shell.execute_reply.started":"2022-05-17T20:03:16.366641Z","shell.execute_reply":"2022-05-17T20:03:21.613585Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('store_sales: \\n', store_sales.shape)\ndisplay(store_sales.head())\nmean_by_family_all_years = store_sales.groupby(['family', 'date']).mean()\nmean_by_family_all_years = mean_by_family_all_years.unstack('family')['sales']\nprint(mean_by_family_all_years.shape)\nmean_by_family_all_years.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:25.597557Z","iopub.execute_input":"2022-05-17T20:03:25.597843Z","iopub.status.idle":"2022-05-17T20:03:25.927274Z","shell.execute_reply.started":"2022-05-17T20:03:25.597813Z","shell.execute_reply":"2022-05-17T20:03:25.926283Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"store_sales: \n (3000888, 1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                 sales\nstore_nbr family     date             \n1         AUTOMOTIVE 2013-01-01    0.0\n                     2013-01-02    2.0\n                     2013-01-03    3.0\n                     2013-01-04    3.0\n                     2013-01-05    5.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>sales</th>\n    </tr>\n    <tr>\n      <th>store_nbr</th>\n      <th>family</th>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1</th>\n      <th rowspan=\"5\" valign=\"top\">AUTOMOTIVE</th>\n      <th>2013-01-01</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-02</th>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2013-01-05</th>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"(1684, 33)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"family      AUTOMOTIVE  BABY CARE    BEAUTY    BEVERAGES  BOOKS  BREAD/BAKERY  \\\ndate                                                                            \n2013-01-01    0.000000        0.0  0.037037    15.000000    0.0      3.344241   \n2013-01-02    4.722222        0.0  3.833333  1335.036987    0.0    486.042938   \n2013-01-03    2.981482        0.0  2.314815   964.907410    0.0    341.786682   \n2013-01-04    3.129630        0.0  2.462963  1003.092590    0.0    309.666077   \n2013-01-05    6.333333        0.0  3.537037  1441.074097    0.0    414.217804   \n\nfamily      CELEBRATION     CLEANING       DAIRY        DELI  ...  MAGAZINES  \\\ndate                                                          ...              \n2013-01-01          0.0     3.444444    2.648148    1.316481  ...        0.0   \n2013-01-02          0.0  1382.018555  432.981476  291.750000  ...        0.0   \n2013-01-03          0.0  1035.055542  333.351837  206.897308  ...        0.0   \n2013-01-04          0.0   964.148132  336.074066  187.837204  ...        0.0   \n2013-01-05          0.0  1298.666626  427.444458  254.350830  ...        0.0   \n\nfamily           MEATS  PERSONAL CARE  PET SUPPLIES  PLAYERS AND ELECTRONICS  \\\ndate                                                                           \n2013-01-01    2.051870       0.462963           0.0                      0.0   \n2013-01-02  386.508606     318.592590           0.0                      0.0   \n2013-01-03  307.359222     232.740738           0.0                      0.0   \n2013-01-04  400.480804     209.314819           0.0                      0.0   \n2013-01-05  386.649841     311.462952           0.0                      0.0   \n\nfamily         POULTRY  PREPARED FOODS  PRODUCE  SCHOOL AND OFFICE SUPPLIES  \\\ndate                                                                          \n2013-01-01    0.789574        0.700870      0.0                         0.0   \n2013-01-02  258.812683       98.853928      0.0                         0.0   \n2013-01-03  197.673965       66.507187      0.0                         0.0   \n2013-01-04  199.491013       82.832703      0.0                         0.0   \n2013-01-05  249.537201      107.964317      0.0                         0.0   \n\nfamily        SEAFOOD  \ndate                   \n2013-01-01   0.000000  \n2013-01-02  28.273148  \n2013-01-03  20.265018  \n2013-01-04  23.946686  \n2013-01-05  23.067352  \n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>family</th>\n      <th>AUTOMOTIVE</th>\n      <th>BABY CARE</th>\n      <th>BEAUTY</th>\n      <th>BEVERAGES</th>\n      <th>BOOKS</th>\n      <th>BREAD/BAKERY</th>\n      <th>CELEBRATION</th>\n      <th>CLEANING</th>\n      <th>DAIRY</th>\n      <th>DELI</th>\n      <th>...</th>\n      <th>MAGAZINES</th>\n      <th>MEATS</th>\n      <th>PERSONAL CARE</th>\n      <th>PET SUPPLIES</th>\n      <th>PLAYERS AND ELECTRONICS</th>\n      <th>POULTRY</th>\n      <th>PREPARED FOODS</th>\n      <th>PRODUCE</th>\n      <th>SCHOOL AND OFFICE SUPPLIES</th>\n      <th>SEAFOOD</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-01</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.037037</td>\n      <td>15.000000</td>\n      <td>0.0</td>\n      <td>3.344241</td>\n      <td>0.0</td>\n      <td>3.444444</td>\n      <td>2.648148</td>\n      <td>1.316481</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2.051870</td>\n      <td>0.462963</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.789574</td>\n      <td>0.700870</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2013-01-02</th>\n      <td>4.722222</td>\n      <td>0.0</td>\n      <td>3.833333</td>\n      <td>1335.036987</td>\n      <td>0.0</td>\n      <td>486.042938</td>\n      <td>0.0</td>\n      <td>1382.018555</td>\n      <td>432.981476</td>\n      <td>291.750000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>386.508606</td>\n      <td>318.592590</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>258.812683</td>\n      <td>98.853928</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>28.273148</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>2.981482</td>\n      <td>0.0</td>\n      <td>2.314815</td>\n      <td>964.907410</td>\n      <td>0.0</td>\n      <td>341.786682</td>\n      <td>0.0</td>\n      <td>1035.055542</td>\n      <td>333.351837</td>\n      <td>206.897308</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>307.359222</td>\n      <td>232.740738</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>197.673965</td>\n      <td>66.507187</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>20.265018</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>3.129630</td>\n      <td>0.0</td>\n      <td>2.462963</td>\n      <td>1003.092590</td>\n      <td>0.0</td>\n      <td>309.666077</td>\n      <td>0.0</td>\n      <td>964.148132</td>\n      <td>336.074066</td>\n      <td>187.837204</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>400.480804</td>\n      <td>209.314819</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>199.491013</td>\n      <td>82.832703</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>23.946686</td>\n    </tr>\n    <tr>\n      <th>2013-01-05</th>\n      <td>6.333333</td>\n      <td>0.0</td>\n      <td>3.537037</td>\n      <td>1441.074097</td>\n      <td>0.0</td>\n      <td>414.217804</td>\n      <td>0.0</td>\n      <td>1298.666626</td>\n      <td>427.444458</td>\n      <td>254.350830</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>386.649841</td>\n      <td>311.462952</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>249.537201</td>\n      <td>107.964317</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>23.067352</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Time features","metadata":{}},{"cell_type":"code","source":"df_target_mean_2017 =  mean_by_family_all_years.loc['2017']\n\nfourier = CalendarFourier(freq=\"M\", order=4) \ndp = DeterministicProcess(\n    index=df_target_mean_2017.index,\n    constant=True,\n    order=1,\n    # YOUR CODE HERE\n    seasonal=True, \n    additional_terms=[fourier],\n    drop=True,\n)\nx_base =  dp.in_sample()\nx_base['new_year'] = (x_base.index.dayofyear == 1)*1.0\nx_base.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:32.509950Z","iopub.execute_input":"2022-05-17T20:03:32.510254Z","iopub.status.idle":"2022-05-17T20:03:32.673425Z","shell.execute_reply.started":"2022-05-17T20:03:32.510225Z","shell.execute_reply":"2022-05-17T20:03:32.672619Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(227, 17)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"# machine learning model\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\n#hyper parameter search\nimport skopt\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_convergence\nfrom skopt.plots import plot_objective, plot_evaluations\nfrom skopt.plots import plot_histogram, plot_objective_2D\nfrom skopt.utils import use_named_args\nimport gc\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:38.060632Z","iopub.execute_input":"2022-05-17T20:03:38.060912Z","iopub.status.idle":"2022-05-17T20:03:39.288419Z","shell.execute_reply.started":"2022-05-17T20:03:38.060882Z","shell.execute_reply":"2022-05-17T20:03:39.287386Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Define the space of search for each hyperparameter\ndim_learning_rate = Real(low=1e-4, high=0.1, prior='log-uniform', name='learning_rate')\ndim_max_depth = Categorical(categories=[5, 9, 10, 14], name='max_depth') #Real(low=0.1, high=0.9, name='rho_value')\ndim_min_child_weight = Categorical(categories=[1, 5, 6, 10], name='min_child_weight')\n\ndim_subsample = Real(low=0.1, high=1.0, name='subsample') \ndim_colsample_bytree = Real(low=0.1, high=1.0, name='colsample_bytree') \ndim_reg_alpha = Real(low=0.1, high=0.8, name='reg_alpha')\ndim_n_estimators = Categorical(categories=[30, 50, 70, 100, 150], name='n_estimators')\n\ndimensions = [dim_learning_rate, dim_max_depth, dim_min_child_weight, dim_subsample, dim_colsample_bytree,\n             dim_reg_alpha, dim_n_estimators]\n\ndef train_model(param_tmp, x_train, y_train, x_val, y_val):\n    model = XGBRegressor(learning_rate = param_tmp['learning_rate'], \n                         max_depth = param_tmp['max_depth'],\n                         min_child_weight = param_tmp['min_child_weight'],\n                         subsample = param_tmp['subsample'],\n                         colsample_bytree = param_tmp['colsample_bytree'],\n                         reg_alpha = param_tmp['reg_alpha'],\n                         n_estimators = param_tmp['n_estimators']) \n    \n    model.fit(x_train, y_train)\n\n    y_pred_train = model.predict(x_train)  \n    y_pred_val = model.predict(x_val)               \n\n    df_rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train, multioutput='raw_values'))\n    df_rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val, multioutput='raw_values'))   \n    \n    return model, df_rmse_train, df_rmse_val\n\niteration = 0\n@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate, max_depth, min_child_weight, subsample, colsample_bytree, reg_alpha, n_estimators):   \n    global iteration   \n    global list_metrics_results\n    \n    def fitness_function(param_tmp):                              \n        num_partition = 0         \n        list_rmse_val = []        \n        tscv = TimeSeriesSplit(n_splits=3, max_train_size=None, test_size=15, gap=0)    \n        for index_train, index_val in tscv.split(np.arange(x_train.shape[0])): \n            \n            print('------------------')               \n            print('__ partition:', num_partition,'__')\n            print('index_train:', index_train[0], 'to', index_train[-1])\n            print('index_val:',  index_val[0], 'to', index_val[-1])            \n            print('split train, val :', index_train.shape, index_val.shape)\n            print('------------------')   \n                    \n            model, df_rmse_train, df_rmse_val = train_model(param_tmp, \n                                                            x_train.iloc[index_train, :], y_train.iloc[index_train, :], \n                                                            x_train.iloc[index_val, :], y_train.iloc[index_val, :])                        \n            print('rmse train:', df_rmse_train.mean(),'\\n', \n                  'rmse val:', df_rmse_val.mean(), '\\n')\n                                           \n            list_rmse_val.append(df_rmse_val.mean())                                 \n            num_partition += 1 \n            \n        IPython.display.clear_output()\n        \n        weights = np.array([0.6, 0.3, 0.1])        \n        mean_rmse = np.round(np.mean(list_rmse_val), 2)\n        weight_mean_rmse = np.round(np.mean(np.array(list_rmse_val)*weights), 2)\n        \n        print(\"*** ---- Hyperparameters ---- ***\")\n        print(param_tmp)\n        print('metrics:')\n        print('mean_rmse:', mean_rmse, 'weight_mean_rmse:', weight_mean_rmse)                                   \n        list_metrics_results.append({'mean_rmse':np.round(np.mean(list_rmse_val), 2), \n                                     'weight_mean_rmse':weight_mean_rmse,\n                                     'cv_rmse_val':list_rmse_val}) \n        return weight_mean_rmse #mean_rmse\n\n    print(\"____________Iteration:\", iteration, \"___________\")\n    \n    param_tmp = {'learning_rate':learning_rate, \n              'max_depth': max_depth,  \n              'min_child_weight':min_child_weight, \n              'subsample': subsample, \n              'colsample_bytree': colsample_bytree, \n              'reg_alpha': reg_alpha,\n              'n_estimators': n_estimators} \n        \n    accuracy = fitness_function(param_tmp)    \n\n    iteration += 1 \n    gc.collect()\n\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:41.935711Z","iopub.execute_input":"2022-05-17T20:03:41.936188Z","iopub.status.idle":"2022-05-17T20:03:41.967295Z","shell.execute_reply.started":"2022-05-17T20:03:41.936148Z","shell.execute_reply":"2022-05-17T20:03:41.966531Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## split data\nsize_test = 15\nx_train = x_base.iloc[:-size_test] \ny_train = df_target_mean_2017.iloc[:-size_test] \n\nx_test = x_base.iloc[-size_test:]\ny_test = df_target_mean_2017.iloc[-size_test:]\n    \nprint(\"all data:\", x_base.shape, \"\\ntrain:\", x_train.shape, y_train.shape, \n      \"\\ntest:\", x_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:44.246828Z","iopub.execute_input":"2022-05-17T20:03:44.247123Z","iopub.status.idle":"2022-05-17T20:03:44.255556Z","shell.execute_reply.started":"2022-05-17T20:03:44.247092Z","shell.execute_reply":"2022-05-17T20:03:44.254594Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"all data: (227, 17) \ntrain: (212, 17) (212, 33) \ntest: (15, 17) (15, 33)\n","output_type":"stream"}]},{"cell_type":"code","source":"iteration = 0\nlist_metrics_results = []\ndefault_parameters = [0.1, 9, 6, 0.8, 0.8, 0.1, 50]\nsearch_result = gp_minimize(func=fitness,\n                            dimensions=dimensions,\n                            acq_func='EI', # Expected Improvement.\n                            n_calls=30,\n                            x0=default_parameters)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:03:58.750784Z","iopub.execute_input":"2022-05-17T20:03:58.751665Z","iopub.status.idle":"2022-05-17T20:06:33.781806Z","shell.execute_reply.started":"2022-05-17T20:03:58.751617Z","shell.execute_reply":"2022-05-17T20:06:33.780385Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"*** ---- Hyperparameters ---- ***\n{'learning_rate': 0.0369596950323374, 'max_depth': 14, 'min_child_weight': 10, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'n_estimators': 30}\nmetrics:\nmean_rmse: 168.34 weight_mean_rmse: 55.86\n","output_type":"stream"}]},{"cell_type":"code","source":"plot_convergence(search_result)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:07:23.463430Z","iopub.execute_input":"2022-05-17T20:07:23.463787Z","iopub.status.idle":"2022-05-17T20:07:23.890033Z","shell.execute_reply.started":"2022-05-17T20:07:23.463739Z","shell.execute_reply":"2022-05-17T20:07:23.889017Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"gp_results = pd.DataFrame(np.hstack([search_result.func_vals.reshape(-1,1), np.array(search_result.x_iters)])).sort_values(0)\ngp_results.columns = ['metric', 'learning_rate', 'max_depth', 'min_child_weight', 'subsample', \n                      'colsample_bytree', 'reg_alpha', 'n_estimators']\ngp_results['metric'] = gp_results['metric'].astype(np.float32)\ngp_results = pd.concat([gp_results, pd.DataFrame(list_metrics_results)], axis=1)\ngp_results = gp_results.sort_values('metric')\ngp_results","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:07:54.857148Z","iopub.execute_input":"2022-05-17T20:07:54.857458Z","iopub.status.idle":"2022-05-17T20:07:54.913428Z","shell.execute_reply.started":"2022-05-17T20:07:54.857427Z","shell.execute_reply":"2022-05-17T20:07:54.912319Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"        metric  learning_rate  max_depth  min_child_weight  subsample  \\\n22   16.290001       0.043005       14.0              10.0   1.000000   \n17   16.719999       0.047660       10.0              10.0   1.000000   \n12   16.920000       0.032995        5.0               6.0   1.000000   \n0    16.980000       0.100000        9.0               6.0   0.800000   \n27   17.010000       0.049739        5.0              10.0   1.000000   \n20   17.219999       0.071049       14.0              10.0   0.849097   \n24   17.559999       0.068108        5.0               6.0   1.000000   \n14   17.610001       0.100000        5.0              10.0   1.000000   \n11   17.670000       0.100000        5.0              10.0   1.000000   \n25   18.200001       0.030295       14.0              10.0   1.000000   \n18   18.610001       0.060292        5.0              10.0   0.716360   \n23   18.610001       0.100000        5.0               1.0   0.662448   \n13   19.059999       0.100000        5.0               1.0   1.000000   \n21   19.320000       0.042390       14.0               1.0   1.000000   \n3    20.750000       0.025185        9.0               6.0   0.736324   \n8    22.430000       0.070001       10.0               1.0   0.228908   \n15   28.559999       0.065426       14.0              10.0   1.000000   \n19   35.470001       0.044824       14.0              10.0   0.100000   \n1    35.580002       0.012322        9.0               6.0   0.857814   \n16   35.840000       0.100000        5.0              10.0   0.100000   \n28   36.209999       0.100000       14.0               1.0   0.100000   \n26   44.990002       0.025929        5.0               1.0   0.100000   \n29   55.860001       0.036960       14.0              10.0   1.000000   \n2    56.750000       0.007673        5.0               6.0   0.735497   \n5   100.680000       0.003712       10.0               6.0   0.102276   \n10  120.930000       0.003228        9.0               6.0   0.576210   \n4   122.489998       0.003057        9.0               5.0   0.869408   \n7   131.110001       0.002381       14.0               5.0   0.323029   \n9   158.139999       0.000513        9.0               6.0   0.459326   \n6   158.570007       0.000330       14.0               6.0   0.678033   \n\n    colsample_bytree  reg_alpha  n_estimators   mean_rmse  weight_mean_rmse  \\\n22          0.929520   0.100000         150.0   47.580002             16.29   \n17          0.675666   0.800000         150.0   50.400002             16.72   \n12          1.000000   0.800000         150.0   51.119999             16.92   \n0           0.800000   0.100000          50.0   51.490002             16.98   \n27          1.000000   0.800000         150.0   50.230000             17.01   \n20          1.000000   0.800000         150.0   50.220001             17.22   \n24          0.860763   0.100000         150.0   50.939999             17.56   \n14          1.000000   0.800000         150.0   50.610001             17.61   \n11          1.000000   0.800000          30.0   54.299999             17.67   \n25          0.790531   0.800000         100.0   54.700001             18.20   \n18          0.783533   0.800000         100.0   55.099998             18.61   \n23          0.789793   0.800000         150.0   55.830002             18.61   \n13          1.000000   0.100000          30.0   58.080002             19.06   \n21          0.759686   0.100000         150.0   58.930000             19.32   \n3           0.950979   0.136335         100.0   63.220001             20.75   \n8           0.797215   0.123333          70.0   68.709999             22.43   \n15          0.100000   0.800000         150.0   85.849998             28.56   \n19          0.219271   0.800000         150.0  101.980003             35.47   \n1           0.519080   0.365754         150.0  106.370003             35.58   \n16          1.000000   0.800000          70.0  103.120003             35.84   \n28          0.100000   0.100000          30.0  102.970001             36.21   \n26          1.000000   0.800000          70.0  131.679993             44.99   \n29          1.000000   0.100000          30.0  168.339996             55.86   \n2           0.822278   0.525959         150.0  169.710007             56.75   \n5           0.276842   0.398872         150.0  299.940002            100.68   \n10          0.294127   0.189808         100.0  361.410004            120.93   \n4           0.418773   0.290898         100.0  366.320007            122.49   \n7           0.264540   0.573762         100.0  392.010010            131.11   \n9           0.483236   0.153291          70.0  473.390015            158.14   \n6           0.304052   0.269970         100.0  474.670013            158.57   \n\n                          cv_rmse_val  \n22  [49.510456, 49.193806, 44.045895]  \n17  [49.433388, 51.605694, 50.151886]  \n12   [48.370888, 56.13982, 48.838856]  \n0      [48.84287, 55.293365, 50.3228]  \n27  [50.785538, 52.874733, 47.024208]  \n20   [52.590202, 51.42998, 46.629158]  \n24   [54.22209, 51.431416, 47.153934]  \n14  [54.613777, 51.740765, 45.474983]  \n11    [47.687042, 64.32472, 50.88725]  \n25  [51.117462, 63.207645, 49.766888]  \n18     [55.12101, 58.69281, 51.49319]  \n23    [55.805145, 55.84568, 55.83683]  \n13    [52.315956, 67.93323, 53.99035]  \n21   [55.566345, 62.52292, 58.709934]  \n3     [56.17472, 75.93889, 57.541687]  \n8      [63.160473, 75.51016, 67.4655]  \n15    [83.241516, 91.58986, 82.72953]  \n19  [107.885864, 109.41529, 88.64408]  \n1    [101.24726, 120.97914, 96.89728]  \n16   [109.5882, 108.986176, 90.77324]  \n28   [111.72381, 109.33446, 87.85613]  \n26  [133.18895, 144.33063, 117.51493]  \n29   [161.16196, 182.43028, 161.4158]  \n2   [163.96898, 186.81665, 158.34904]  \n5    [297.0972, 317.50928, 285.22446]  \n10    [356.96964, 379.3355, 347.9371]  \n4    [361.3993, 384.34726, 353.19867]  \n7      [387.5495, 409.75073, 378.725]  \n9    [468.50275, 490.82037, 460.8565]  \n6   [469.77637, 492.08154, 462.13913]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>metric</th>\n      <th>learning_rate</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>subsample</th>\n      <th>colsample_bytree</th>\n      <th>reg_alpha</th>\n      <th>n_estimators</th>\n      <th>mean_rmse</th>\n      <th>weight_mean_rmse</th>\n      <th>cv_rmse_val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>16.290001</td>\n      <td>0.043005</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>0.929520</td>\n      <td>0.100000</td>\n      <td>150.0</td>\n      <td>47.580002</td>\n      <td>16.29</td>\n      <td>[49.510456, 49.193806, 44.045895]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>16.719999</td>\n      <td>0.047660</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>0.675666</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>50.400002</td>\n      <td>16.72</td>\n      <td>[49.433388, 51.605694, 50.151886]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>16.920000</td>\n      <td>0.032995</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>51.119999</td>\n      <td>16.92</td>\n      <td>[48.370888, 56.13982, 48.838856]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>16.980000</td>\n      <td>0.100000</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>0.800000</td>\n      <td>0.800000</td>\n      <td>0.100000</td>\n      <td>50.0</td>\n      <td>51.490002</td>\n      <td>16.98</td>\n      <td>[48.84287, 55.293365, 50.3228]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>17.010000</td>\n      <td>0.049739</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>50.230000</td>\n      <td>17.01</td>\n      <td>[50.785538, 52.874733, 47.024208]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>17.219999</td>\n      <td>0.071049</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>0.849097</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>50.220001</td>\n      <td>17.22</td>\n      <td>[52.590202, 51.42998, 46.629158]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>17.559999</td>\n      <td>0.068108</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>1.000000</td>\n      <td>0.860763</td>\n      <td>0.100000</td>\n      <td>150.0</td>\n      <td>50.939999</td>\n      <td>17.56</td>\n      <td>[54.22209, 51.431416, 47.153934]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>17.610001</td>\n      <td>0.100000</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>50.610001</td>\n      <td>17.61</td>\n      <td>[54.613777, 51.740765, 45.474983]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>17.670000</td>\n      <td>0.100000</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>30.0</td>\n      <td>54.299999</td>\n      <td>17.67</td>\n      <td>[47.687042, 64.32472, 50.88725]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>18.200001</td>\n      <td>0.030295</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>0.790531</td>\n      <td>0.800000</td>\n      <td>100.0</td>\n      <td>54.700001</td>\n      <td>18.20</td>\n      <td>[51.117462, 63.207645, 49.766888]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18.610001</td>\n      <td>0.060292</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>0.716360</td>\n      <td>0.783533</td>\n      <td>0.800000</td>\n      <td>100.0</td>\n      <td>55.099998</td>\n      <td>18.61</td>\n      <td>[55.12101, 58.69281, 51.49319]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>18.610001</td>\n      <td>0.100000</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.662448</td>\n      <td>0.789793</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>55.830002</td>\n      <td>18.61</td>\n      <td>[55.805145, 55.84568, 55.83683]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>19.059999</td>\n      <td>0.100000</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>30.0</td>\n      <td>58.080002</td>\n      <td>19.06</td>\n      <td>[52.315956, 67.93323, 53.99035]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>19.320000</td>\n      <td>0.042390</td>\n      <td>14.0</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.759686</td>\n      <td>0.100000</td>\n      <td>150.0</td>\n      <td>58.930000</td>\n      <td>19.32</td>\n      <td>[55.566345, 62.52292, 58.709934]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20.750000</td>\n      <td>0.025185</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>0.736324</td>\n      <td>0.950979</td>\n      <td>0.136335</td>\n      <td>100.0</td>\n      <td>63.220001</td>\n      <td>20.75</td>\n      <td>[56.17472, 75.93889, 57.541687]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>22.430000</td>\n      <td>0.070001</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>0.228908</td>\n      <td>0.797215</td>\n      <td>0.123333</td>\n      <td>70.0</td>\n      <td>68.709999</td>\n      <td>22.43</td>\n      <td>[63.160473, 75.51016, 67.4655]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>28.559999</td>\n      <td>0.065426</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>85.849998</td>\n      <td>28.56</td>\n      <td>[83.241516, 91.58986, 82.72953]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>35.470001</td>\n      <td>0.044824</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>0.100000</td>\n      <td>0.219271</td>\n      <td>0.800000</td>\n      <td>150.0</td>\n      <td>101.980003</td>\n      <td>35.47</td>\n      <td>[107.885864, 109.41529, 88.64408]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35.580002</td>\n      <td>0.012322</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>0.857814</td>\n      <td>0.519080</td>\n      <td>0.365754</td>\n      <td>150.0</td>\n      <td>106.370003</td>\n      <td>35.58</td>\n      <td>[101.24726, 120.97914, 96.89728]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>35.840000</td>\n      <td>0.100000</td>\n      <td>5.0</td>\n      <td>10.0</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>70.0</td>\n      <td>103.120003</td>\n      <td>35.84</td>\n      <td>[109.5882, 108.986176, 90.77324]</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>36.209999</td>\n      <td>0.100000</td>\n      <td>14.0</td>\n      <td>1.0</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>30.0</td>\n      <td>102.970001</td>\n      <td>36.21</td>\n      <td>[111.72381, 109.33446, 87.85613]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>44.990002</td>\n      <td>0.025929</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.100000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>70.0</td>\n      <td>131.679993</td>\n      <td>44.99</td>\n      <td>[133.18895, 144.33063, 117.51493]</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>55.860001</td>\n      <td>0.036960</td>\n      <td>14.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n      <td>30.0</td>\n      <td>168.339996</td>\n      <td>55.86</td>\n      <td>[161.16196, 182.43028, 161.4158]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>56.750000</td>\n      <td>0.007673</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>0.735497</td>\n      <td>0.822278</td>\n      <td>0.525959</td>\n      <td>150.0</td>\n      <td>169.710007</td>\n      <td>56.75</td>\n      <td>[163.96898, 186.81665, 158.34904]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>100.680000</td>\n      <td>0.003712</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>0.102276</td>\n      <td>0.276842</td>\n      <td>0.398872</td>\n      <td>150.0</td>\n      <td>299.940002</td>\n      <td>100.68</td>\n      <td>[297.0972, 317.50928, 285.22446]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>120.930000</td>\n      <td>0.003228</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>0.576210</td>\n      <td>0.294127</td>\n      <td>0.189808</td>\n      <td>100.0</td>\n      <td>361.410004</td>\n      <td>120.93</td>\n      <td>[356.96964, 379.3355, 347.9371]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>122.489998</td>\n      <td>0.003057</td>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>0.869408</td>\n      <td>0.418773</td>\n      <td>0.290898</td>\n      <td>100.0</td>\n      <td>366.320007</td>\n      <td>122.49</td>\n      <td>[361.3993, 384.34726, 353.19867]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>131.110001</td>\n      <td>0.002381</td>\n      <td>14.0</td>\n      <td>5.0</td>\n      <td>0.323029</td>\n      <td>0.264540</td>\n      <td>0.573762</td>\n      <td>100.0</td>\n      <td>392.010010</td>\n      <td>131.11</td>\n      <td>[387.5495, 409.75073, 378.725]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>158.139999</td>\n      <td>0.000513</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>0.459326</td>\n      <td>0.483236</td>\n      <td>0.153291</td>\n      <td>70.0</td>\n      <td>473.390015</td>\n      <td>158.14</td>\n      <td>[468.50275, 490.82037, 460.8565]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>158.570007</td>\n      <td>0.000330</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>0.678033</td>\n      <td>0.304052</td>\n      <td>0.269970</td>\n      <td>100.0</td>\n      <td>474.670013</td>\n      <td>158.57</td>\n      <td>[469.77637, 492.08154, 462.13913]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Final evaluation over the test using the top-10 hp\ndef final_evaluation(n, gp_results):\n    list_results = []        \n    for i in range(n):        \n        best_hp = gp_results.iloc[i, 1:8].to_dict()\n        \n        best_hp['max_depth'] = int(best_hp['max_depth'])\n        best_hp['n_estimators'] = int(best_hp['n_estimators'])\n        print('top:',i, 'param:', best_hp)\n        model, df_rmse_train, df_rmse_test = train_model(best_hp, x_train, y_train, x_test, y_test)\n        print('rmse train:', df_rmse_train.mean(),' rmse test:', df_rmse_test.mean(),'\\n')        \n        list_results.append({'rmse train':df_rmse_train.mean(), 'rmse test':df_rmse_test.mean()})\n    return list_results","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:08:08.302998Z","iopub.execute_input":"2022-05-17T20:08:08.303341Z","iopub.status.idle":"2022-05-17T20:08:08.311291Z","shell.execute_reply.started":"2022-05-17T20:08:08.303308Z","shell.execute_reply":"2022-05-17T20:08:08.310164Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"list_results = final_evaluation(30, gp_results)","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:09:31.385795Z","iopub.execute_input":"2022-05-17T20:09:31.386135Z","iopub.status.idle":"2022-05-17T20:10:20.142063Z","shell.execute_reply.started":"2022-05-17T20:09:31.386096Z","shell.execute_reply":"2022-05-17T20:10:20.141450Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"top: 0 param: {'learning_rate': 0.04300454784710047, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 0.9295203065568836, 'reg_alpha': 0.1, 'n_estimators': 150}\nrmse train: 41.281487  rmse test: 58.066936 \n\ntop: 1 param: {'learning_rate': 0.04766022814822882, 'max_depth': 10, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 0.6756657952731185, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 41.390533  rmse test: 59.839447 \n\ntop: 2 param: {'learning_rate': 0.032994864059755755, 'max_depth': 5, 'min_child_weight': 6.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 48.24993  rmse test: 53.35472 \n\ntop: 3 param: {'learning_rate': 0.1, 'max_depth': 9, 'min_child_weight': 6.0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'n_estimators': 50}\nrmse train: 43.03627  rmse test: 56.682602 \n\ntop: 4 param: {'learning_rate': 0.04973940115489217, 'max_depth': 5, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 46.656223  rmse test: 56.77722 \n\ntop: 5 param: {'learning_rate': 0.0710494798781977, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 0.8490968449425594, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 34.568787  rmse test: 58.44857 \n\ntop: 6 param: {'learning_rate': 0.06810839641014312, 'max_depth': 5, 'min_child_weight': 6.0, 'subsample': 1.0, 'colsample_bytree': 0.8607631609223324, 'reg_alpha': 0.1, 'n_estimators': 150}\nrmse train: 37.085552  rmse test: 57.293133 \n\ntop: 7 param: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 36.453938  rmse test: 58.643764 \n\ntop: 8 param: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 30}\nrmse train: 65.16862  rmse test: 48.88393 \n\ntop: 9 param: {'learning_rate': 0.030295099163186144, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 0.7905309185850791, 'reg_alpha': 0.8, 'n_estimators': 100}\nrmse train: 63.5225  rmse test: 48.09002 \n\ntop: 10 param: {'learning_rate': 0.06029164949194182, 'max_depth': 5, 'min_child_weight': 10.0, 'subsample': 0.7163599754268567, 'colsample_bytree': 0.7835325908032797, 'reg_alpha': 0.8, 'n_estimators': 100}\nrmse train: 53.87765  rmse test: 59.278015 \n\ntop: 11 param: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1.0, 'subsample': 0.6624479209777913, 'colsample_bytree': 0.7897927694529288, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 10.240794  rmse test: 61.88674 \n\ntop: 12 param: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'n_estimators': 30}\nrmse train: 49.205967  rmse test: 51.01755 \n\ntop: 13 param: {'learning_rate': 0.04238984944667948, 'max_depth': 14, 'min_child_weight': 1.0, 'subsample': 1.0, 'colsample_bytree': 0.7596860284137763, 'reg_alpha': 0.1, 'n_estimators': 150}\nrmse train: 5.763161  rmse test: 50.356182 \n\ntop: 14 param: {'learning_rate': 0.02518523309950011, 'max_depth': 9, 'min_child_weight': 6.0, 'subsample': 0.7363244169963298, 'colsample_bytree': 0.9509794595004165, 'reg_alpha': 0.1363345656881822, 'n_estimators': 100}\nrmse train: 72.90752  rmse test: 47.967052 \n\ntop: 15 param: {'learning_rate': 0.07000124404672155, 'max_depth': 10, 'min_child_weight': 1.0, 'subsample': 0.22890794347450993, 'colsample_bytree': 0.7972147139562192, 'reg_alpha': 0.1233327628176635, 'n_estimators': 70}\nrmse train: 55.06472  rmse test: 48.07468 \n\ntop: 16 param: {'learning_rate': 0.0654257139224348, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 0.1, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 81.430855  rmse test: 72.38596 \n\ntop: 17 param: {'learning_rate': 0.04482397230236666, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 0.1, 'colsample_bytree': 0.21927142647126888, 'reg_alpha': 0.8, 'n_estimators': 150}\nrmse train: 110.973404  rmse test: 75.31027 \n\ntop: 18 param: {'learning_rate': 0.012321559545177121, 'max_depth': 9, 'min_child_weight': 6.0, 'subsample': 0.8578143257080729, 'colsample_bytree': 0.5190797418493559, 'reg_alpha': 0.3657538128980552, 'n_estimators': 150}\nrmse train: 112.99185  rmse test: 81.05091 \n\ntop: 19 param: {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 10.0, 'subsample': 0.1, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 70}\nrmse train: 109.60795  rmse test: 75.320465 \n\ntop: 20 param: {'learning_rate': 0.1, 'max_depth': 14, 'min_child_weight': 1.0, 'subsample': 0.1, 'colsample_bytree': 0.1, 'reg_alpha': 0.1, 'n_estimators': 30}\nrmse train: 111.53244  rmse test: 74.57738 \n\ntop: 21 param: {'learning_rate': 0.02592867197691337, 'max_depth': 5, 'min_child_weight': 1.0, 'subsample': 0.1, 'colsample_bytree': 1.0, 'reg_alpha': 0.8, 'n_estimators': 70}\nrmse train: 130.75677  rmse test: 98.09108 \n\ntop: 22 param: {'learning_rate': 0.0369596950323374, 'max_depth': 14, 'min_child_weight': 10.0, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.1, 'n_estimators': 30}\nrmse train: 178.76726  rmse test: 151.48962 \n\ntop: 23 param: {'learning_rate': 0.007672624410273629, 'max_depth': 5, 'min_child_weight': 6.0, 'subsample': 0.7354969844189215, 'colsample_bytree': 0.8222778536526101, 'reg_alpha': 0.525958960657758, 'n_estimators': 150}\nrmse train: 178.68768  rmse test: 149.60869 \n\ntop: 24 param: {'learning_rate': 0.0037118725245921668, 'max_depth': 10, 'min_child_weight': 6.0, 'subsample': 0.10227581449703614, 'colsample_bytree': 0.27684179593042, 'reg_alpha': 0.3988717812083399, 'n_estimators': 150}\nrmse train: 305.33884  rmse test: 277.1166 \n\ntop: 25 param: {'learning_rate': 0.003228089944381482, 'max_depth': 9, 'min_child_weight': 6.0, 'subsample': 0.5762096012401778, 'colsample_bytree': 0.2941270116766921, 'reg_alpha': 0.18980803582310812, 'n_estimators': 100}\nrmse train: 366.8928  rmse test: 341.17682 \n\ntop: 26 param: {'learning_rate': 0.0030573206038215974, 'max_depth': 9, 'min_child_weight': 5.0, 'subsample': 0.8694080327302759, 'colsample_bytree': 0.4187729379295029, 'reg_alpha': 0.29089840373891696, 'n_estimators': 100}\nrmse train: 371.79062  rmse test: 346.6122 \n\ntop: 27 param: {'learning_rate': 0.0023809886076343175, 'max_depth': 14, 'min_child_weight': 5.0, 'subsample': 0.3230291984717695, 'colsample_bytree': 0.26454004973999906, 'reg_alpha': 0.5737622002052969, 'n_estimators': 100}\nrmse train: 397.1512  rmse test: 371.93457 \n\ntop: 28 param: {'learning_rate': 0.0005132576767664527, 'max_depth': 9, 'min_child_weight': 6.0, 'subsample': 0.4593263500306748, 'colsample_bytree': 0.4832356118944465, 'reg_alpha': 0.15329127323577435, 'n_estimators': 70}\nrmse train: 478.06592  rmse test: 454.36502 \n\ntop: 29 param: {'learning_rate': 0.0003299158114004909, 'max_depth': 14, 'min_child_weight': 6.0, 'subsample': 0.678032548134446, 'colsample_bytree': 0.30405204221896187, 'reg_alpha': 0.26997026921362066, 'n_estimators': 100}\nrmse train: 479.3063  rmse test: 455.6248 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"## higher 10\npd.DataFrame(list_results[0:10])","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:10:40.833859Z","iopub.execute_input":"2022-05-17T20:10:40.834606Z","iopub.status.idle":"2022-05-17T20:10:40.846016Z","shell.execute_reply.started":"2022-05-17T20:10:40.834569Z","shell.execute_reply":"2022-05-17T20:10:40.844842Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   rmse train  rmse test\n0   41.281487  58.066936\n1   41.390533  59.839447\n2   48.249931  53.354721\n3   43.036270  56.682602\n4   46.656223  56.777222\n5   34.568787  58.448570\n6   37.085552  57.293133\n7   36.453938  58.643764\n8   65.168617  48.883930\n9   63.522499  48.090019","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rmse train</th>\n      <th>rmse test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41.281487</td>\n      <td>58.066936</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41.390533</td>\n      <td>59.839447</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>48.249931</td>\n      <td>53.354721</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43.036270</td>\n      <td>56.682602</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46.656223</td>\n      <td>56.777222</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>34.568787</td>\n      <td>58.448570</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>37.085552</td>\n      <td>57.293133</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>36.453938</td>\n      <td>58.643764</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>65.168617</td>\n      <td>48.883930</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>63.522499</td>\n      <td>48.090019</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## lower 10\npd.DataFrame(list_results[-10:])","metadata":{"execution":{"iopub.status.busy":"2022-05-17T20:10:58.176360Z","iopub.execute_input":"2022-05-17T20:10:58.177005Z","iopub.status.idle":"2022-05-17T20:10:58.187972Z","shell.execute_reply.started":"2022-05-17T20:10:58.176963Z","shell.execute_reply":"2022-05-17T20:10:58.186818Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   rmse train   rmse test\n0  111.532440   74.577377\n1  130.756775   98.091080\n2  178.767258  151.489624\n3  178.687683  149.608688\n4  305.338837  277.116608\n5  366.892792  341.176819\n6  371.790619  346.612213\n7  397.151215  371.934570\n8  478.065918  454.365021\n9  479.306305  455.624786","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rmse train</th>\n      <th>rmse test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>111.532440</td>\n      <td>74.577377</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>130.756775</td>\n      <td>98.091080</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>178.767258</td>\n      <td>151.489624</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>178.687683</td>\n      <td>149.608688</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>305.338837</td>\n      <td>277.116608</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>366.892792</td>\n      <td>341.176819</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>371.790619</td>\n      <td>346.612213</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>397.151215</td>\n      <td>371.934570</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>478.065918</td>\n      <td>454.365021</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>479.306305</td>\n      <td>455.624786</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}